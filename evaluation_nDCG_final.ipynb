{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team Project: Web Search and Information Retrieval - Topic 4 Effiecient Vector Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR_v31_evaluation - 05/26/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** Test VSM retrieval model on **doc_dump.txt** text collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Retrieval Performance on Full Document Collection - 'doc_dump.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Here we use doc_dump.txt which means the unpreprocessed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw train.docs text and split\n",
    "raw_texts = open('data/raw/doc_dump.txt', encoding=\"utf-8\").read()\n",
    "doc_list = raw_texts.split(\"\\n\")\n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show typical example of document in doc_dump.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MED-2',\n",
       " 'http://www.ncbi.nlm.nih.gov/pubmed/22809476',\n",
       " 'A statistical regression model for the estimation of acrylamide concentrations in French fries for excess lifetime cancer risk assessment. - PubMed - NCBI',\n",
       " 'Abstract Human exposure to acrylamide (AA) through consumption of French fries and other foods has been recognized as a potential health concern. Here, we used a statistical non-linear regression model, based on the two most influential factors, cooking temperature and time, to estimate AA concentrations in French fries. The R(2) of the predictive model is 0.83, suggesting the developed model was significant and valid. Based on French fry intake survey data conducted in this study and eight frying temperature-time schemes which can produce tasty and visually appealing French fries, the Monte Carlo simulation results showed that if AA concentration is higher than 168 ppb, the estimated cancer risk for adolescents aged 13-18 years in Taichung City would be already higher than the target excess lifetime cancer risk (ELCR), and that by taking into account this limited life span only. In order to reduce the cancer risk associated with AA intake, the AA levels in French fries might have to be reduced even further if the epidemiological observations are valid. Our mathematical model can serve as basis for further investigations on ELCR including different life stages and behavior and population groups. Copyright Â© 2012 Elsevier Ltd. All rights reserved.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show typical example of document in doc_dump.txt\n",
    "doc_list[1].split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document collection D\n",
    "doc_collection = dict()\n",
    "\n",
    "for i in range(len(doc_list)):\n",
    "    list_ = doc_list[i].split(\"\\t\")\n",
    "    if len(list_) == 4:\n",
    "        key_ = list_[0]\n",
    "        title_ = list_[2]\n",
    "        text_ = list_[3]\n",
    "        value_ = title_ + \" \" + text_\n",
    "        doc_collection.update({key_: value_})\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use same list of documents that were used for testing in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load list of documents used for testing\n",
    "raw_texts = open('data/raw/test.docs.ids').read()\n",
    "test_list = raw_texts.split(\"\\n\")\n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the last element in list is simply an empty string and will be removed\n",
    "test_list[3162]\n",
    "test_list.pop()\n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create test document collection that will be used to compute speed, MAP\n",
    "doc_collection_test = dict()\n",
    "for idx in doc_collection.keys():\n",
    "    # print(idx)\n",
    "    if idx in test_list:\n",
    "        key_ = idx\n",
    "        value_ = doc_collection[idx]\n",
    "        doc_collection_test.update({key_: value_})\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "len(doc_collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete full doc_collection to avoid confusion!\n",
    "del doc_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *LOAD OWN IMPLEMENTTAION OF VECTOR SPACE RETRIEVAL FUNCTIONS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/roman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load own implementations of VSM\n",
    "from VSM_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *INDEXING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idf computation done in 16.6076s.\n"
     ]
    }
   ],
   "source": [
    "# compute global idf scores on D\n",
    "idfs = compute_Idf(doc_collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf computation done in 22.7820s.\n"
     ]
    }
   ],
   "source": [
    "# Compute tfidf scores\n",
    "tfidfs = compute_TfIdf(doc_collection_test, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvertedIndex construction done in 0.2316s.\n"
     ]
    }
   ],
   "source": [
    "# Construct inverted index\n",
    "inverted_index = construct_invertedIndex(doc_collection_test, idfs, tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM construction done in 7.7208s.\n"
     ]
    }
   ],
   "source": [
    "# construct term document matrix\n",
    "tdm = create_tdm(doc_collection_test, tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocLength index construction done in 0.1292s.\n"
     ]
    }
   ],
   "source": [
    "# Compute dict that stores vector norm length for each document d in D\n",
    "doc_lengths = construct_docLengthDict(doc_collection_test, tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preclustering done in 22.2298s.\n"
     ]
    }
   ],
   "source": [
    "# Compute preclustering with sqrt(n) random leaders\n",
    "clusters = pre_cluster(doc_collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TieredIndex construction done in 0.2153s.\n"
     ]
    }
   ],
   "source": [
    "# Construct tiered index\n",
    "tiered_index = construct_tiered_index(doc_collection_test, inverted_index, t=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full indexing takes ca. 1min - 2min. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *QUERYING*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in query collection - Using train.nontopic-titles.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw text and split\n",
    "raw_texts = open('data/test/test.nontopic-titles.queries', encoding=\"utf-8\").read()\n",
    "query_list = raw_texts.split(\"\\n\")\n",
    "len(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary that holds the queryID + queryText\n",
    "query_col = dict()\n",
    "\n",
    "for i in range(len(query_list)):\n",
    "    if len(query_list[i].split(\"\\t\")) == 2:\n",
    "        key_, value_ = query_list[i].split(\"\\t\")\n",
    "        query_col.update({key_ : value_})\n",
    "        \n",
    "len(query_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a typical non.topic page title resembles a query an average user would type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do vegetarians get enough protein ?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display an example query\n",
    "query_col[\"PLAIN-2590\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in gold standard for relevance judgements - Using test.2-1-0.qrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12335"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw text and split\n",
    "raw_texts = open('data/test/test.2-1-0.qrel').read()\n",
    "rel_list = raw_texts.split(\"\\n\")\n",
    "len(rel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary that holds queryID + emptyList for all queries in Q\n",
    "gold_col = dict()\n",
    "\n",
    "for i in range(len(rel_list)):\n",
    "    list_ = rel_list[i].split(\"\\t\")\n",
    "    if len(list_)==4:\n",
    "        key_ = list_[0]\n",
    "        value_ = list()\n",
    "        if key_ in query_col.keys():\n",
    "            gold_col.update({key_ : value_})\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(rel_list)):\n",
    "    list_ = rel_list[i].split(\"\\t\")\n",
    "    if len(list_)==4:\n",
    "        key_ = list_[0]\n",
    "        docID_ = list_[2]\n",
    "        score_ = list_[3]\n",
    "        if key_ in gold_col.keys():\n",
    "            tuple_ = (docID_, int(score_))\n",
    "            gold_col[key_].append(tuple_)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "# show length of gold_col\n",
    "len(gold_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MED-2288', 2),\n",
       " ('MED-3137', 2),\n",
       " ('MED-2290', 2),\n",
       " ('MED-2291', 2),\n",
       " ('MED-2292', 2),\n",
       " ('MED-2293', 2),\n",
       " ('MED-2294', 2),\n",
       " ('MED-2295', 2),\n",
       " ('MED-2296', 2),\n",
       " ('MED-2498', 1),\n",
       " ('MED-2517', 1),\n",
       " ('MED-2519', 1),\n",
       " ('MED-2501', 1),\n",
       " ('MED-2502', 1),\n",
       " ('MED-2513', 1),\n",
       " ('MED-2504', 1),\n",
       " ('MED-2505', 1),\n",
       " ('MED-2506', 1),\n",
       " ('MED-2507', 1),\n",
       " ('MED-5239', 1),\n",
       " ('MED-2509', 1),\n",
       " ('MED-2510', 1),\n",
       " ('MED-2511', 1),\n",
       " ('MED-2512', 1),\n",
       " ('MED-3000', 1),\n",
       " ('MED-2765', 1),\n",
       " ('MED-2997', 1),\n",
       " ('MED-3001', 1),\n",
       " ('MED-2999', 1),\n",
       " ('MED-4313', 1),\n",
       " ('MED-3148', 1),\n",
       " ('MED-3149', 1),\n",
       " ('MED-3242', 1),\n",
       " ('MED-3243', 1),\n",
       " ('MED-3244', 1),\n",
       " ('MED-3245', 1),\n",
       " ('MED-3270', 1),\n",
       " ('MED-3271', 1),\n",
       " ('MED-3272', 1),\n",
       " ('MED-3273', 1),\n",
       " ('MED-3274', 1),\n",
       " ('MED-3275', 1),\n",
       " ('MED-3276', 1),\n",
       " ('MED-3277', 1),\n",
       " ('MED-3278', 1),\n",
       " ('MED-3279', 1),\n",
       " ('MED-3280', 1),\n",
       " ('MED-3281', 1),\n",
       " ('MED-3282', 1),\n",
       " ('MED-3283', 1),\n",
       " ('MED-3580', 1),\n",
       " ('MED-3581', 1),\n",
       " ('MED-3582', 1),\n",
       " ('MED-3583', 1),\n",
       " ('MED-3584', 1),\n",
       " ('MED-3858', 1),\n",
       " ('MED-4094', 1),\n",
       " ('MED-3860', 1),\n",
       " ('MED-3862', 1),\n",
       " ('MED-4107', 1),\n",
       " ('MED-4299', 1),\n",
       " ('MED-4298', 1),\n",
       " ('MED-4600', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show list of relevant documents + relevance judgements for PLAIN-2590\n",
    "gold_col[\"PLAIN-2590\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we now have ordered relevance judgements for all queries in our test collection we can measure the nDCG score for our retrieval systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nDCG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: 'vanilla' (using Term-Document Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Normalized Discounted Cumulative Gain (nDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate nDCG \n",
    "nDCG_list = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, TDM=tdm, idfDict=idfs,\n",
    "                        D = doc_collection_test, k = 3162, strategy=\"vanilla\",\n",
    "                        show_documents=False, print_scores=False,\n",
    "                       return_results=True, return_speed=False)\n",
    "    \n",
    "    qScore = evaluate_nDCG(y_pred=topK_scores, y_true=gold_list, variant=\"raw_scores\")\n",
    "    nDCG_list.append(qScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.4749\n"
     ]
    }
   ],
   "source": [
    "nDCG_avg = np.mean(nDCG_list)\n",
    "print(\"nDCG: {:.4f}\".format(nDCG_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: 'optimal' (using invertedIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Normalized Discounted Cumulative Gain (nDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate nDCG \n",
    "nDCG_list = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162,\n",
    "                                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                lengthIdx = doc_lengths,\n",
    "                                show_documents=False, return_results=True, print_scores=False)\n",
    "    \n",
    "    qScore = evaluate_nDCG(y_pred=topK_scores, y_true=gold_list, variant=\"raw_scores\")\n",
    "    nDCG_list.append(qScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.4749\n"
     ]
    }
   ],
   "source": [
    "nDCG_avg = np.mean(nDCG_list)\n",
    "print(\"nDCG: {:.4f}\".format(nDCG_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: 'postingMerge Intersection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Normalized Discounted Cumulative Gain (nDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate nDCG \n",
    "nDCG_list = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"intersection\",\n",
    "                                    idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                    lengthIdx = doc_lengths,\n",
    "                                    show_documents=False, print_scores=False,\n",
    "                                    return_results=True, return_speed=False)\n",
    "    \n",
    "    qScore = evaluate_nDCG(y_pred=topK_scores, y_true=gold_list, variant=\"raw_scores\")\n",
    "    nDCG_list.append(qScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.2993\n"
     ]
    }
   ],
   "source": [
    "nDCG_avg = np.mean(nDCG_list)\n",
    "print(\"nDCG: {:.4f}\".format(nDCG_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 4: 'preclustering'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Normalized Discounted Cumulative Gain (nDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate nDCG \n",
    "nDCG_list = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"preclustering\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, preClusterDict=clusters,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    qScore = evaluate_nDCG(y_pred=topK_scores, y_true=gold_list, variant=\"raw_scores\")\n",
    "    nDCG_list.append(qScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.3171\n"
     ]
    }
   ],
   "source": [
    "nDCG_avg = np.mean(nDCG_list)\n",
    "print(\"nDCG: {:.4f}\".format(nDCG_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 5: 'tiered_index' with t = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Normalized Discounted Cumulative Gain (nDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate nDCG \n",
    "nDCG_list = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"tiered\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    qScore = evaluate_nDCG(y_pred=topK_scores, y_true=gold_list, variant=\"raw_scores\")\n",
    "    nDCG_list.append(qScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.4711\n"
     ]
    }
   ],
   "source": [
    "nDCG_avg = np.mean(nDCG_list)\n",
    "print(\"nDCG: {:.4f}\".format(nDCG_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 5b: 'tiered_index' with t=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TieredIndex construction done in 0.2104s.\n"
     ]
    }
   ],
   "source": [
    "# Construct tiered index\n",
    "tiered_index = construct_tiered_index(doc_collection_test, inverted_index, t=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Normalized Discounted Cumulative Gain (nDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate nDCG \n",
    "nDCG_list = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"tiered\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    qScore = evaluate_nDCG(y_pred=topK_scores, y_true=gold_list, variant=\"raw_scores\")\n",
    "    nDCG_list.append(qScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.4626\n"
     ]
    }
   ],
   "source": [
    "nDCG_avg = np.mean(nDCG_list)\n",
    "print(\"nDCG: {:.4f}\".format(nDCG_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
