{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team Project: Web Search and Information Retrieval - Topic 4 Effiecient Vector Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Retrieval Time & MAP Evaluation Notebook -- last updated 05/26/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Purpose:** Test VSM retrieval model on **doc_dump.txt** text collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "- 1. Evaluate 'vanilla'\n",
    "- 2. Evaluate 'optimal'/ 'invertedIndex'\n",
    "- 3. Evaluate 'postingMergeIntersection'\n",
    "- 4. Evaluate 'preclustering'\n",
    "- 5. Evaluate 'tieredIndex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Retrieval Performance on Full Document Collection - 'doc_dump.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Here we use doc_dump.txt which means the raw/unpreprocessed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw train.docs text and split\n",
    "raw_texts = open('data/raw/doc_dump.txt', encoding=\"utf-8\").read()\n",
    "doc_list = raw_texts.split(\"\\n\")\n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show typical example of document in doc_dump.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MED-2',\n",
       " 'http://www.ncbi.nlm.nih.gov/pubmed/22809476',\n",
       " 'A statistical regression model for the estimation of acrylamide concentrations in French fries for excess lifetime cancer risk assessment. - PubMed - NCBI',\n",
       " 'Abstract Human exposure to acrylamide (AA) through consumption of French fries and other foods has been recognized as a potential health concern. Here, we used a statistical non-linear regression model, based on the two most influential factors, cooking temperature and time, to estimate AA concentrations in French fries. The R(2) of the predictive model is 0.83, suggesting the developed model was significant and valid. Based on French fry intake survey data conducted in this study and eight frying temperature-time schemes which can produce tasty and visually appealing French fries, the Monte Carlo simulation results showed that if AA concentration is higher than 168 ppb, the estimated cancer risk for adolescents aged 13-18 years in Taichung City would be already higher than the target excess lifetime cancer risk (ELCR), and that by taking into account this limited life span only. In order to reduce the cancer risk associated with AA intake, the AA levels in French fries might have to be reduced even further if the epidemiological observations are valid. Our mathematical model can serve as basis for further investigations on ELCR including different life stages and behavior and population groups. Copyright © 2012 Elsevier Ltd. All rights reserved.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show typical example of document in doc_dump.txt\n",
    "doc_list[1].split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document collection D\n",
    "doc_collection = dict()\n",
    "\n",
    "for i in range(len(doc_list)):\n",
    "    list_ = doc_list[i].split(\"\\t\")\n",
    "    if len(list_) == 4:\n",
    "        key_ = list_[0]\n",
    "        title_ = list_[2]\n",
    "        text_ = list_[3]\n",
    "        value_ = title_ + \" \" + text_\n",
    "        doc_collection.update({key_: value_})\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use same list of documents that were used for testing in original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load list of documents used for testing\n",
    "raw_texts = open('data/raw/test.docs.ids').read()\n",
    "test_list = raw_texts.split(\"\\n\")\n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the last element in list is simply an empty string and will be removed\n",
    "test_list[3162]\n",
    "test_list.pop()\n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create test document collection that will be used to compute speed, MAP\n",
    "doc_collection_test = dict()\n",
    "for idx in doc_collection.keys():\n",
    "    # print(idx)\n",
    "    if idx in test_list:\n",
    "        key_ = idx\n",
    "        value_ = doc_collection[idx]\n",
    "        doc_collection_test.update({key_: value_})\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "len(doc_collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete full doc_collection to avoid confusion!\n",
    "del doc_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *LOAD OWN IMPLEMENTTAION OF VECTOR SPACE RETRIEVAL FUNCTIONS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/roman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load own implementations of VSM\n",
    "from VSM_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *INDEXING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idf computation done in 18.0459s.\n"
     ]
    }
   ],
   "source": [
    "# compute global idf scores on D\n",
    "idfs = compute_Idf(doc_collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf computation done in 24.4949s.\n"
     ]
    }
   ],
   "source": [
    "# Compute tfidf scores\n",
    "tfidfs = compute_TfIdf(doc_collection_test, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvertedIndex construction done in 0.2234s.\n"
     ]
    }
   ],
   "source": [
    "# Construct inverted index\n",
    "inverted_index = construct_invertedIndex(doc_collection_test, idfs, tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM construction done in 8.9105s.\n"
     ]
    }
   ],
   "source": [
    "# construct term document matrix\n",
    "tdm = create_tdm(doc_collection_test, tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocLength index construction done in 0.1214s.\n"
     ]
    }
   ],
   "source": [
    "# Compute dict that stores vector norm length for each document d in D\n",
    "doc_lengths = construct_docLengthDict(doc_collection_test, tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preclustering done in 22.6574s.\n"
     ]
    }
   ],
   "source": [
    "# Compute preclustering with sqrt(n) random leaders\n",
    "clusters = pre_cluster(doc_collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TieredIndex construction done in 0.2100s.\n"
     ]
    }
   ],
   "source": [
    "# Construct tiered index\n",
    "tiered_index = construct_tiered_index(doc_collection_test, inverted_index, t=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full indexing takes ca. 1min - 2min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *QUERYING*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in query collection - Using test.nontopic-titles.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we use nontopic-titles.queries. Same as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw text and split\n",
    "raw_texts = open('data/test/test.nontopic-titles.queries', encoding=\"utf-8\").read()\n",
    "query_list = raw_texts.split(\"\\n\")\n",
    "len(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary that holds the queryID + queryText\n",
    "query_col = dict()\n",
    "\n",
    "for i in range(len(query_list)):\n",
    "    if len(query_list[i].split(\"\\t\")) == 2:\n",
    "        key_, value_ = query_list[i].split(\"\\t\")\n",
    "        query_col.update({key_ : value_})\n",
    "        \n",
    "len(query_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a typical non.topic page title resembles a query an average user would type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do vegetarians get enough protein ?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display an example query\n",
    "query_col[\"PLAIN-2590\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in gold standard for relevance judgements - Using test.2-1-0.qrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12335"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw text and split\n",
    "raw_texts = open('data/test/test.2-1-0.qrel').read()\n",
    "rel_list = raw_texts.split(\"\\n\")\n",
    "len(rel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary that holds queryID + emptyList for all queries in Q\n",
    "gold_col = dict()\n",
    "\n",
    "for i in range(len(rel_list)):\n",
    "    list_ = rel_list[i].split(\"\\t\")\n",
    "    if len(list_)==4:\n",
    "        key_ = list_[0]\n",
    "        value_ = list()\n",
    "        if key_ in query_col.keys():\n",
    "            gold_col.update({key_ : value_})\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(rel_list)):\n",
    "    list_ = rel_list[i].split(\"\\t\")\n",
    "    if len(list_)==4:\n",
    "        key_ = list_[0]\n",
    "        value_ = list_[2]\n",
    "        if key_ in gold_col.keys():\n",
    "            gold_col[key_].append(value_)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "# show length of gold_col\n",
    "len(gold_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show relevant documents for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MED-2288',\n",
       " 'MED-3137',\n",
       " 'MED-2290',\n",
       " 'MED-2291',\n",
       " 'MED-2292',\n",
       " 'MED-2293',\n",
       " 'MED-2294',\n",
       " 'MED-2295',\n",
       " 'MED-2296',\n",
       " 'MED-2498',\n",
       " 'MED-2517',\n",
       " 'MED-2519',\n",
       " 'MED-2501',\n",
       " 'MED-2502',\n",
       " 'MED-2513',\n",
       " 'MED-2504',\n",
       " 'MED-2505',\n",
       " 'MED-2506',\n",
       " 'MED-2507',\n",
       " 'MED-5239',\n",
       " 'MED-2509',\n",
       " 'MED-2510',\n",
       " 'MED-2511',\n",
       " 'MED-2512',\n",
       " 'MED-3000',\n",
       " 'MED-2765',\n",
       " 'MED-2997',\n",
       " 'MED-3001',\n",
       " 'MED-2999',\n",
       " 'MED-4313',\n",
       " 'MED-3148',\n",
       " 'MED-3149',\n",
       " 'MED-3242',\n",
       " 'MED-3243',\n",
       " 'MED-3244',\n",
       " 'MED-3245',\n",
       " 'MED-3270',\n",
       " 'MED-3271',\n",
       " 'MED-3272',\n",
       " 'MED-3273',\n",
       " 'MED-3274',\n",
       " 'MED-3275',\n",
       " 'MED-3276',\n",
       " 'MED-3277',\n",
       " 'MED-3278',\n",
       " 'MED-3279',\n",
       " 'MED-3280',\n",
       " 'MED-3281',\n",
       " 'MED-3282',\n",
       " 'MED-3283',\n",
       " 'MED-3580',\n",
       " 'MED-3581',\n",
       " 'MED-3582',\n",
       " 'MED-3583',\n",
       " 'MED-3584',\n",
       " 'MED-3858',\n",
       " 'MED-4094',\n",
       " 'MED-3860',\n",
       " 'MED-3862',\n",
       " 'MED-4107',\n",
       " 'MED-4299',\n",
       " 'MED-4298',\n",
       " 'MED-4600']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show list of relevant documents for MED-3254\n",
    "# To measure MAP all documents in this list are considered to be of equal relevance\n",
    "# See nDCG evaluation for ordered relevance judgments!\n",
    "gold_col[\"PLAIN-2590\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use example query to show topK retrieval, where k = 10\n",
    "- loop 10 times over all documents in query test collection to measure retrieval speed (i.e. 1440 iterations)\n",
    "- loop over all documents in test collection to measure MAP, nDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do vegetarians get enough protein ?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_col[\"PLAIN-2590\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = query_col[\"PLAIN-2590\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: 'vanilla' (using Term-Document Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topK document retrieval with k=10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show / return documents once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Retrieval time ca. 0.50370526 seconds.\n",
      "Highest cosine similarity:\n",
      "\tMED-4984 : 0.23064\n",
      "\tMED-5340 : 0.22193\n",
      "\tMED-1771 : 0.20181\n",
      "\tMED-4163 : 0.19685\n",
      "\tMED-1540 : 0.19446\n",
      "\tMED-1541 : 0.19444\n",
      "\tMED-1135 : 0.18747\n",
      "\tMED-2294 : 0.17669\n",
      "\tMED-2939 : 0.17560\n",
      "\tMED-2290 : 0.17405\n",
      "==================================================\n",
      "\n",
      "Vegetarian and vegan diets in type 2 diabetes management. - PubMed - NCBI Abstract Vegetarian and vegan diets offer significant benefits for diabetes management. In observational studies, individuals following vegetarian diets are about half as likely to develop diabetes, compared with non-vegetarians. In clinical trials in individuals with type 2 diabetes, low-fat vegan diets improve glycemic control to a greater extent than conventional diabetes diets. Although this effect is primarily attributable to greater weight loss, evidence also suggests that reduced intake of saturated fats and high-glycemic-index foods, increased intake of dietary fiber and vegetable protein, reduced intramyocellular lipid concentrations, and decreased iron stores mediate the influence of plant-based diets on glycemia. Vegetarian and vegan diets also improve plasma lipid concentrations and have been shown to reverse atherosclerosis progression. In clinical studies, the reported acceptability of vegetarian and vegan diets is comparable to other therapeutic regimens. The presently available literature indicates that vegetarian and vegan diets present potential advantages for the management of type 2 diabetes.\n",
      "\n",
      "Renal function parameters of Thai vegans compared with non-vegans. - PubMed - NCBI Abstract In Asia, vegetarianism is a well-established eating behavior. It appears that the adoption of a vegan diet leads to a lessening of several health risk factors. Although vegetarianism has some notable effects on the hematological system, the effect on the nephrological system has not been well clarified. The pattern of renal function parameters was studied in 25 Thai vegans compared with 25 non-vegetarians. Of the studied parameters, it was found that urine protein was significantly different (p < 0.05) in vegans and controls. Vegans had significantly lower urine protein level.\n",
      "\n",
      "Some observations on human semen analysis. - PubMed - NCBI Abstract Semen analysis of 66 unmarried medical students in the age group of 17-21 years was carried out. A higher liquefaction time pH, motility, lower sperm count and abnormal forms were observed compared to reported values. Liquefaction time, pH and sperm count was found significantly different in non-vegetarians and vegetarians, perhaps due to difference in their dietary proteins.\n",
      "\n",
      "Protein dietary reference intakes may be inadequate for vegetarians if low amounts of animal protein are consumed. - PubMed - NCBI Abstract OBJECTIVE: The health benefits of vegetarian diets are well-recognized; however, long-term adherence to these diets may be associated with nutrient inadequacies, particularly vitamins B12 and D, calcium, iron, zinc, and protein. The dietary reference intakes (DRIs) expert panels recommended adjustments to the iron, zinc, and calcium DRIs for vegetarians to account for decreased bioavailability, but no adjustments were considered necessary for the protein DRI under the assumption that vegetarians consume about 50% of protein from animal (dairy/egg) sources. This study examined dietary protein sources in a convenience sample of 21 young adult vegetarian women who completed food logs on 4 consecutive days (3 weekdays and 1 weekend day). METHODS: The daily contribution percentages of protein consumed from cereals, legumes, nuts/seeds, fruits/vegetables, and dairy/egg were computed, and the protein digestibility corrected amino acid score of the daily diets was calculated. RESULTS: The calculated total dietary protein digestibility score for participants was 82 ± 1%, which differed significantly (P < 0.001) from the DRI reference score, 88%, and the 4-d average protein digestibility corrected amino acid score for the sample was 80 ± 2%, which also differed significantly (P < 0.001) from the DRI reference value, 100%. The analyses indicated that animal protein accounted for only 21% of dietary protein. CONCLUSION: This research suggests that the protein DRI for vegetarians consuming less than the expected amounts of animal protein (45% to 50% of total protein) may need to be adjusted from 0.8 to about 1.0 g/kg to account for decreased protein bioavailability. Copyright © 2011 Elsevier Inc. All rights reserved.\n",
      "\n",
      "Vegetarian diets: what do we know of their effects on common chronic diseases? Abstract A number of studies have evaluated the health of vegetarians. Others have studied the health effects of foods that are preferred or avoided by vegetarians. The purpose of this review is to look critically at the evidence on the health effects of vegetarian diets and to seek possible explanations where results appear to conflict. There is convincing evidence that vegetarians have lower rates of coronary heart disease, largely explained by low LDL cholesterol, probable lower rates of hypertension and diabetes mellitus, and lower prevalence of obesity. Overall, their cancer rates appear to be moderately lower than others living in the same communities, and life expectancy appears to be greater. However, results for specific cancers are much less convincing and require more study. There is evidence that risk of colorectal cancer is lower in vegetarians and in those who eat less meat; however, results from British vegetarians presently disagree, and this needs explanation. It is probable that using the label “vegetarian” as a dietary category is too broad and that our understanding will be served well by dividing vegetarians into more descriptive subtypes. Although vegetarian diets are healthful and are associated with lower risk of several chronic diseases, different types of vegetarians may not experience the same effects on health.\n",
      "\n",
      "Does a vegetarian diet reduce the occurrence of diabetes? Abstract We propose the hypothesis that a vegetarian diet reduces the risk of developing diabetes. Findings that have generated this hypothesis are from a population of 25,698 adult White Seventh-day Adventists identified in 1960. During 21 years of follow-up, the risk of diabetes as an underlying cause of death in Adventists was approximately one-half the risk for all US Whites. Within the male Adventist population, vegetarians had a substantially lower risk than non-vegetarians of diabetes as an underlying or contributing cause of death. Within both the male and female Adventist populations, the prevalence of self-reported diabetes also was lower in vegetarians than in non-vegetarians. The associations observed between diabetes and meat consumption were apparently not due to confounding by over- or under-weight, other selected dietary factors, or physical activity. All of the associations between meat consumption and diabetes were stronger in males than in females.\n",
      "\n",
      "Should recurrent calcium oxalate stone formers become vegetarians? - PubMed - NCBI Abstract The hypothesis that the incidence of calcium stone disease is related to the consumption of animal protein has been examined. Within the male population, recurrent idiopathic stone formers consumed more animal protein than did normal subjects. Single stone formers had animal protein intakes intermediate between those of normal men and those of recurrent stone formers. A high animal protein intake caused a significant increase in the urinary excretion of calcium, oxalate and uric acid, 3 of the 6 main urinary risk factors for calcium stone formation. The overall relative probability of forming stones, calculated from the combination of the 6 main urinary risk factors, was markedly increased by a high animal protein diet. Conversely, a low animal protein intake, such as taken by vegetarians, was associated with a low excretion of calcium, oxalate and uric acid and a low relative probability of forming stones.\n",
      "\n",
      "Comparison of Nutritional Quality of the Vegan, Vegetarian, Semi-Vegetarian, Pesco-Vegetarian and Omnivorous Diet Abstract The number of studies comparing nutritional quality of restrictive diets is limited. Data on vegan subjects are especially lacking. It was the aim of the present study to compare the quality and the contributing components of vegan, vegetarian, semi-vegetarian, pesco-vegetarian and omnivorous diets. Dietary intake was estimated using a cross-sectional online survey with a 52-items food frequency questionnaire (FFQ). Healthy Eating Index 2010 (HEI-2010) and the Mediterranean Diet Score (MDS) were calculated as indicators for diet quality. After analysis of the diet questionnaire and the FFQ, 1475 participants were classified as vegans (n = 104), vegetarians (n = 573), semi-vegetarians (n = 498), pesco-vegetarians (n = 145), and omnivores (n = 155). The most restricted diet, i.e., the vegan diet, had the lowest total energy intake, better fat intake profile, lowest protein and highest dietary fiber intake in contrast to the omnivorous diet. Calcium intake was lowest for the vegans and below national dietary recommendations. The vegan diet received the highest index values and the omnivorous the lowest for HEI-2010 and MDS. Typical aspects of a vegan diet (high fruit and vegetable intake, low sodium intake, and low intake of saturated fat) contributed substantially to the total score, independent of the indexing system used. The score for the more prudent diets (vegetarians, semi-vegetarians and pesco-vegetarians) differed as a function of the used indexing system but they were mostly better in terms of nutrient quality than the omnivores.\n",
      "\n",
      "Arterial function of carotid and brachial arteries in postmenopausal vegetarians Abstract Background: Vegetarianism is associated with a lower risk of cardiovascular disease. However, studies of arterial function in vegetarians are limited. Methods: This study investigated arterial function in vegetarianism by comparing 49 healthy postmenopausal vegetarians with 41 age-matched omnivores. The arterial function of the common carotid artery was assessed by carotid duplex, while the pulse dynamics method was used to measure brachial artery distensibility (BAD), compliance (BAC), and resistance (BAR). Fasting blood levels of glucose, lipids, lipoprotein (a), high-sensitivity C-reactive protein, homocysteine, and vitamin B12 were also measured. Results: Vegetarians had significantly lower serum cholesterol, high-density and low-density lipoprotein, and glucose compared with omnivores. They also had lower vitamin B12 but higher homocysteine levels. Serum levels of lipoprotein (a) and high-sensitivity C-reactive protein were no different between the two groups. There were no significant differences in carotid beta stiffness index, BAC, and BAD between the two groups even after adjustment for associated covariates. However, BAR was significantly lower in vegetarians than in omnivores. Multiple linear regression analysis revealed that age and pulse pressure were two important determinants of carotid beta stiffness index and BAD. Vegetarianism is not associated with better arterial elasticity. Conclusion: Apparently healthy postmenopausal vegetarians are not significantly better in terms of carotid beta stiffness index, BAC, and BAD, but have significantly decreased BAR than omnivores. Prevention of vitamin B12 deficiency might be beneficial for cardiovascular health in vegetarians.\n",
      "\n",
      "Nutrient Profiles of Vegetarian and Non Vegetarian Dietary Patterns Abstract Background Differences in nutrient profiles between vegetarian and non vegetarian dietary patterns reflect nutritional differences that may contribute to the development of disease. Objective To compare nutrient intakes between dietary patterns characterized by consumption or exclusion of meat and dairy products. Design Cross-sectional study of 71751 subjects (mean age 59 years) from the Adventist-Health-Study-2. Data was collected between 2002 and 2007. Participants completed a 204-item validated semi-quantitative food frequency questionnaire. Dietary patterns compared were: non vegetarian, semi vegetarian, pesco vegetarian, lacto-ovo vegetarian and strict vegetarian. ANCOVA was used to analyze differences in nutrient intakes by dietary patterns and were adjusted for age, and sex and race. BMI and other relevant demographic data were reported and compared by dietary pattern using chi-square tests and ANOVA. Results Many nutrient intakes varied significantly between dietary patterns. Non vegetarians had the lowest intakes of plant proteins, fiber, β-Carotene, and Mg than those following vegetarian dietary patterns and the highest intakes of saturated, trans, arachidonic, and docosahexaenoic fatty acids. The lower tails of some nutrient distributions in strict vegetarians suggested inadequate intakes by a portion of the subjects. Energy intake was similar among dietary patterns at close to 2000 kcal/d with the exception of semi vegetarians that had an intake of 1713 kcal/d. Mean BMI was highest in non-vegetarians (mean; standard deviation [SD]) (28.7; [6.4]) and lowest in strict vegetarians (24.0; [4.8]). Conclusions Nutrient profiles varied markedly between dietary patterns that were defined by meat and dairy intakes. These differences can be of interest in the etiology of obesity and chronic diseases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_retrieval(q = query1, TDM=tdm, idfDict=idfs,\n",
    "                D = doc_collection_test, k = 10, strategy=\"vanilla\",\n",
    "                show_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct evaluation means to rank all 3612 documents!'\n",
    "len(doc_collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MAP \n",
    "AP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, TDM=tdm, idfDict=idfs,\n",
    "                        D = doc_collection_test, k = 3162, strategy=\"vanilla\",\n",
    "                        show_documents=False, print_scores=False,\n",
    "                       return_results=True, return_speed=False)\n",
    "    \n",
    "    avg_precision = evaluate_AveragePrecision(y_pred=topK_scores, y_true=gold_list)\n",
    "    AP.append(avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.1447\n"
     ]
    }
   ],
   "source": [
    "MAP = np.mean(AP)\n",
    "print(\"MAP: {:.4f}\".format(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - r-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MAP \n",
    "RP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list = gold_col[qIDX]\n",
    "    K = len(gold_list)\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, TDM=tdm, idfDict=idfs,\n",
    "                        D = doc_collection_test, k = K, strategy=\"vanilla\",\n",
    "                        show_documents=False, print_scores=False,\n",
    "                       return_results=True, return_speed=False)\n",
    "    \n",
    "    r_precision = evaluate_pAtRank(y_pred=topK_scores, y_true=gold_list, atRank=K)\n",
    "    RP.append(r_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-precision: 0.1608\n"
     ]
    }
   ],
   "source": [
    "RP_avg = np.mean(RP)\n",
    "print(\"R-precision: {:.4f}\".format(RP_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    for qIDX, qTEXT in query_col.items():\n",
    "        query = qTEXT\n",
    "\n",
    "        topK_scores, speed = top_k_retrieval(q = query, TDM=tdm, idfDict=idfs,\n",
    "                            D = doc_collection_test, k = 10, strategy=\"vanilla\",\n",
    "                            show_documents=False, print_scores=False,\n",
    "                           return_results=True, return_speed=True)\n",
    "\n",
    "\n",
    "        speed_list.append(speed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time in sec. (average over 10 iterations): 0.5623\n"
     ]
    }
   ],
   "source": [
    "speed_avg = np.mean(speed_list)\n",
    "print(\"Retrieval time in sec. (average over 10 iterations): {:.4f}\".format(speed_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: 'standard' (using invertedIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topK document retrieval with k=10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: No documents are shown here to keep the notebook more clearly arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Retrieval time ca. 0.00351310 seconds.\n",
      "Highest cosine similarity:\n",
      "\tMED-4984 : 0.36616\n",
      "\tMED-5340 : 0.35233\n",
      "\tMED-1771 : 0.32038\n",
      "\tMED-4163 : 0.31251\n",
      "\tMED-1540 : 0.30872\n",
      "\tMED-1541 : 0.30870\n",
      "\tMED-1135 : 0.29762\n",
      "\tMED-2294 : 0.28051\n",
      "\tMED-2939 : 0.27878\n",
      "\tMED-2290 : 0.27632\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_retrieval(q = query1, D = doc_collection_test, k = 10, strategy=\"standard\",\n",
    "                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                lengthIdx = doc_lengths,\n",
    "                show_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MAP \n",
    "AP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162,\n",
    "                                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                lengthIdx = doc_lengths,\n",
    "                                show_documents=False, return_results=True, print_scores=False)\n",
    "    \n",
    "    avg_precision = evaluate_AveragePrecision(y_pred=topK_scores, y_true=gold_list)\n",
    "    AP.append(avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.1447\n"
     ]
    }
   ],
   "source": [
    "MAP = np.mean(AP)\n",
    "print(\"MAP: {:.4f}\".format(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - r-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MAP \n",
    "RP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    K = len(gold_list)\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = K,\n",
    "                                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                lengthIdx = doc_lengths,\n",
    "                                show_documents=False, return_results=True, print_scores=False)\n",
    "    \n",
    "    r_precision = evaluate_pAtRank(y_pred=topK_scores, y_true=gold_list, atRank=K)\n",
    "    RP.append(r_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-precision: 0.1608\n"
     ]
    }
   ],
   "source": [
    "RP_avg = np.mean(RP)\n",
    "print(\"R-precision: {:.4f}\".format(RP_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    for qIDX, qTEXT in query_col.items():\n",
    "        query = qTEXT\n",
    "\n",
    "        topK_scores, speed = top_k_retrieval(q = query, D = doc_collection_test, k = 10,\n",
    "                            idfDict = idfs, invertedIdx = inverted_index,\n",
    "                            lengthIdx = doc_lengths,\n",
    "                            show_documents=False, print_scores=False,\n",
    "                                             return_results=True, return_speed=True\n",
    "                                            )\n",
    "        \n",
    "        speed_list.append(speed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time in sec. (avg. over 10 iterations): 0.0030\n"
     ]
    }
   ],
   "source": [
    "speed_avg = np.mean(speed_list)\n",
    "print(\"Retrieval time in sec. (avg. over 10 iterations): {:.4f}\".format(speed_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: 'postingMerge Intersection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topK document retrieval with k=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Retrieval time ca. 0.00361586 seconds.\n",
      "Highest cosine similarity:\n",
      "\tMED-4984 : 0.36616\n",
      "\tMED-5340 : 0.35233\n",
      "\tMED-1771 : 0.32038\n",
      "\tMED-4163 : 0.31251\n",
      "\tMED-1135 : 0.29762\n",
      "\tMED-2294 : 0.28051\n",
      "\tMED-2939 : 0.27878\n",
      "\tMED-2290 : 0.27632\n",
      "\tMED-1723 : 0.26479\n",
      "\tMED-1613 : 0.25572\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_retrieval(q = query1, D = doc_collection_test, k = 10, strategy=\"intersection\",\n",
    "                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                lengthIdx = doc_lengths,\n",
    "                show_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Mean Average precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate MAP \n",
    "AP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"intersection\",\n",
    "                                    idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                    lengthIdx = doc_lengths,\n",
    "                                    show_documents=False, print_scores=False,\n",
    "                                    return_results=True, return_speed=False)\n",
    "    \n",
    "    avg_precision = evaluate_AveragePrecision(y_pred=topK_scores, y_true=gold_list)\n",
    "    AP.append(avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.0329\n"
     ]
    }
   ],
   "source": [
    "MAP = np.mean(AP)\n",
    "print(\"MAP: {:.4f}\".format(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - r-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate r-precision \n",
    "RP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    K = len(gold_list)\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = K, strategy=\"intersection\",\n",
    "                                    idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                    lengthIdx = doc_lengths,\n",
    "                                    show_documents=False, print_scores=False,\n",
    "                                    return_results=True, return_speed=False)\n",
    "    \n",
    "    r_precision = evaluate_pAtRank(y_pred=topK_scores, y_true=gold_list, atRank=K)\n",
    "    RP.append(r_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-precision: 0.0266\n"
     ]
    }
   ],
   "source": [
    "RP_avg = np.mean(RP)\n",
    "print(\"R-precision: {:.4f}\".format(RP_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    for qIDX, qTEXT in query_col.items():\n",
    "        query = qTEXT\n",
    "\n",
    "        topK_scores, speed = top_k_retrieval(q = query, D = doc_collection_test, k = 10,\n",
    "                                             strategy=\"intersection\",\n",
    "                                            idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                            lengthIdx = doc_lengths,\n",
    "                                            show_documents=False, print_scores=False,\n",
    "                                            return_results=True, return_speed=True)\n",
    "\n",
    "        speed_list.append(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time in sec. (avg. over 10 iterations): 0.0029\n"
     ]
    }
   ],
   "source": [
    "speed_avg = np.mean(speed_list)\n",
    "print(\"Retrieval time in sec. (avg. over 10 iterations): {:.4f}\".format(speed_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 4: 'preclustering'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topK document retrieval with k=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Retrieval time ca. 0.02604771 seconds.\n",
      "Highest cosine similarity:\n",
      "\tMED-5340 : 0.35233\n",
      "\tMED-4163 : 0.31251\n",
      "\tMED-1541 : 0.30870\n",
      "\tMED-2294 : 0.28051\n",
      "\tMED-2290 : 0.27632\n",
      "\tMED-3236 : 0.27344\n",
      "\tMED-3779 : 0.24989\n",
      "\tMED-4637 : 0.24899\n",
      "\tMED-3557 : 0.24485\n",
      "\tMED-1530 : 0.22907\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_retrieval(q = query1, D = doc_collection_test, k = 10, strategy=\"preclustering\",\n",
    "                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                lengthIdx = doc_lengths, preClusterDict=clusters,\n",
    "                show_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Mean Average precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MAP\n",
    "AP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"preclustering\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, preClusterDict=clusters,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    avg_precision = evaluate_AveragePrecision(y_pred=topK_scores, y_true=gold_list)\n",
    "    AP.append(avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.0638\n"
     ]
    }
   ],
   "source": [
    "MAP = np.mean(AP)\n",
    "print(\"MAP: {:.4f}\".format(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - r-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate r-precision\n",
    "RP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    K = len(gold_list)\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = K, strategy=\"preclustering\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, preClusterDict=clusters,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    r_precision = evaluate_pAtRank(y_pred=topK_scores, y_true=gold_list, atRank=K)\n",
    "    RP.append(r_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-precision: 0.0848\n"
     ]
    }
   ],
   "source": [
    "RP_avg = np.mean(RP)\n",
    "print(\"R-precision: {:.4f}\".format(RP_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    for qIDX, qTEXT in query_col.items():\n",
    "        query = qTEXT\n",
    "\n",
    "        topK_scores, speed = top_k_retrieval(q = query, D = doc_collection_test, k = 10, strategy=\"preclustering\",\n",
    "                                             idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                             lengthIdx = doc_lengths, preClusterDict=clusters,\n",
    "                                             show_documents=False, print_scores=False,\n",
    "                                             return_results=True, return_speed=True)\n",
    "\n",
    "        speed_list.append(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time in sec. (avg. over 10 iterations): 0.0324\n"
     ]
    }
   ],
   "source": [
    "speed_avg = np.mean(speed_list)\n",
    "print(\"Retrieval time in sec. (avg. over 10 iterations): {:.4f}\".format(speed_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 5: 'tiered_index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topK document retrieval with k=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Retrieval time ca. 0.00654340 seconds.\n",
      "Highest cosine similarity:\n",
      "\tMED-5340 : 0.35233\n",
      "\tMED-1771 : 0.32038\n",
      "\tMED-4163 : 0.31251\n",
      "\tMED-1540 : 0.30872\n",
      "\tMED-1541 : 0.30870\n",
      "\tMED-4984 : 0.30004\n",
      "\tMED-1135 : 0.29762\n",
      "\tMED-2939 : 0.27878\n",
      "\tMED-3236 : 0.27344\n",
      "\tMED-1723 : 0.26479\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_retrieval(q = query1, D = doc_collection_test, k = 10, strategy=\"tiered\",\n",
    "                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                lengthIdx = doc_lengths, preClusterDict=clusters, tieredIdx = tiered_index,\n",
    "                show_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Mean Average precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MAP \n",
    "AP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"tiered\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    avg_precision = evaluate_AveragePrecision(y_pred=topK_scores, y_true=gold_list)\n",
    "    AP.append(avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.1432\n"
     ]
    }
   ],
   "source": [
    "MAP = np.mean(AP)\n",
    "print(\"MAP: {:.4f}\".format(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - r-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate r-precision \n",
    "RP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    K = len(gold_list)\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = K, strategy=\"tiered\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    r_precision = evaluate_pAtRank(y_pred=topK_scores, y_true=gold_list, atRank=K)\n",
    "    RP.append(r_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-precision: 0.1601\n"
     ]
    }
   ],
   "source": [
    "RP_avg = np.mean(RP)\n",
    "print(\"R-precision: {:.4f}\".format(RP_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    for qIDX, qTEXT in query_col.items():\n",
    "        query = qTEXT\n",
    "\n",
    "        topK_scores, speed = top_k_retrieval(q = query, D = doc_collection_test, k = 10, strategy=\"tiered\",\n",
    "                                             idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                             lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                             show_documents=False, print_scores=False,\n",
    "                                             return_results=True, return_speed=True)\n",
    "\n",
    "        speed_list.append(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time in sec. (avg. over 10 iterations): 0.0034\n"
     ]
    }
   ],
   "source": [
    "speed_avg = np.mean(speed_list)\n",
    "print(\"Retrieval time in sec. (avg. over 10 iterations): {:.4f}\".format(speed_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 5b: 'tiered_index' t=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TieredIndex construction done in 0.1575s.\n"
     ]
    }
   ],
   "source": [
    "# Construct tiered index\n",
    "tiered_index = construct_tiered_index(doc_collection_test, inverted_index, t=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topK document retrieval with k=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Retrieval time ca. 0.00424910 seconds.\n",
      "Highest cosine similarity:\n",
      "\tMED-4163 : 0.31251\n",
      "\tMED-1540 : 0.30872\n",
      "\tMED-1541 : 0.30870\n",
      "\tMED-4984 : 0.30004\n",
      "\tMED-3236 : 0.27344\n",
      "\tMED-4644 : 0.26043\n",
      "\tMED-5340 : 0.25942\n",
      "\tMED-4632 : 0.25273\n",
      "\tMED-3779 : 0.24989\n",
      "\tMED-4637 : 0.24899\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_retrieval(q = query1, D = doc_collection_test, k = 10, strategy=\"tiered\",\n",
    "                idfDict = idfs, invertedIdx = inverted_index,\n",
    "                lengthIdx = doc_lengths, preClusterDict=clusters, tieredIdx = tiered_index,\n",
    "                show_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Mean Average precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MAP \n",
    "AP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = 3162, strategy=\"tiered\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    avg_precision = evaluate_AveragePrecision(y_pred=topK_scores, y_true=gold_list)\n",
    "    AP.append(avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.1369\n"
     ]
    }
   ],
   "source": [
    "MAP = np.mean(AP)\n",
    "print(\"MAP: {:.4f}\".format(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - r-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate r-precision \n",
    "RP = list()\n",
    "\n",
    "for qIDX, qTEXT in query_col.items():\n",
    "    query = qTEXT\n",
    "    gold_list=gold_col[qIDX]\n",
    "    K = len(gold_list)\n",
    "    \n",
    "    topK_scores = top_k_retrieval(q = query, D = doc_collection_test, k = K, strategy=\"tiered\",\n",
    "                                         idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                         lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                         show_documents=False, print_scores=False,\n",
    "                                         return_results=True, return_speed=False)\n",
    "    \n",
    "    r_precision = evaluate_pAtRank(y_pred=topK_scores, y_true=gold_list, atRank=K)\n",
    "    RP.append(r_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-precision: 0.1571\n"
     ]
    }
   ],
   "source": [
    "RP_avg = np.mean(RP)\n",
    "print(\"R-precision: {:.4f}\".format(RP_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    for qIDX, qTEXT in query_col.items():\n",
    "        query = qTEXT\n",
    "\n",
    "        topK_scores, speed = top_k_retrieval(q = query, D = doc_collection_test, k = 10, strategy=\"tiered\",\n",
    "                                             idfDict = idfs, invertedIdx = inverted_index,\n",
    "                                             lengthIdx = doc_lengths, tieredIdx = tiered_index,\n",
    "                                             show_documents=False, print_scores=False,\n",
    "                                             return_results=True, return_speed=True)\n",
    "\n",
    "        speed_list.append(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time in sec. (avg. over 10 iterations): 0.0029\n"
     ]
    }
   ],
   "source": [
    "speed_avg = np.mean(speed_list)\n",
    "print(\"Retrieval time in sec. (avg. over 10 iterations): {:.4f}\".format(speed_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
